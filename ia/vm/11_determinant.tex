\subsection{Definition}
For an \(n \times n\) matrix \(M\) with columns \(\vb C_a = M\vb e_a\), then the determinant \(\det(M) = \abs{M} \in \mathbb R\) or \(\mathbb C\) is given by any of the following equivalent definitions.
\begin{align*}
	\det M
	 & = [\vb C_1, \vb C_2, \cdots, \vb C_n]                                                \\
	 & = [M\vb e_1, M\vb e_2, \cdots, M\vb e_n]                                             \\
	 & = \varepsilon_{ij\cdots l}M_{i1}M_{j2} \cdots M_{l n}                                \\
	 & = \sum_\sigma \varepsilon(\sigma) M_{\sigma(1)1}M_{\sigma(2)2} \cdots M_{\sigma(n)n}
\end{align*}
Here are some examples.
\begin{enumerate}
	\item \(n=2\)
	      \[
		      \det M = \sum_\sigma M_{\sigma(1)1}M_{\sigma(2)2} = \begin{vmatrix}
			      M_{11} & M_{21} \\ M_{12} & M_{22}
		      \end{vmatrix} = M_{11}M_{22} - M_{12}M_{21}
	      \]
	\item \(M\) diagonal, i.e.\ \(M_{ij} = 0\) for \(i \neq j\)
	      \[
		      M = \begin{pmatrix}
			      M_{11} & 0      & \cdots & 0      \\
			      0      & M_{22} & \cdots & 0      \\
			      \vdots & \vdots & \ddots & \vdots \\
			      0      & 0      & \cdots & M_{nn}
		      \end{pmatrix} \implies \det M = M_{11}M_{22}\cdots M_{nn}
	      \]
	\item Let \(M\) be \(n\times n\), \(A\) be \((n-1) \times (n-1)\), where
	      \[
		      M = \left( \begin{array}{c|c}
				      A & 0 \\\hline
				      0 & 1
			      \end{array} \right)
	      \]
	      We call \(M\) a matrix `in block form'.
	      So \(M_{ni} = M_{in} = 0\) if \(i \neq n\).
	      So we can restrict the permutation \(\sigma\) to only transmuting the first \((n-1)\) terms, i.e.\ \(\sigma(n) = n\).
	      So \(\det M = \det A\).
\end{enumerate}

\begin{proposition}
	If \(\vb R_a\) are the rows of \(M\), \(\det M\) is given by
	\begin{align*}
		\det M
		 & = [\vb R_1, \vb R_2, \cdots, \vb R_n]                                                \\
		 & = \varepsilon_{ij\cdots l}M_{1i}M_{2j} \cdots M_{nl}                                 \\
		 & = \sum_\sigma \varepsilon(\sigma) M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}
	\end{align*}
	i.e.\ \(\det M = \det M^\transpose\).
\end{proposition}
\begin{proof}
	Recall that \((\vb C_a)_i = M_{ia} = (\vb R_i)_a\).
	We need to show that one of these definitions is equivalent to one of the previous definitions, then all other equivalent definitions follow.
	We use the \(\Sigma\) definition by considering the product \(M_{1\sigma(1)}M_{2\sigma(2)} \cdots M_{n\sigma(n)}\).
	We may rewrite this product in a different order: \(M_{\rho(1)1}M_{\rho(2)2} \cdots M_{\rho(n)n}\).
	Then \(\rho = \sigma^{-1}\).
	But then \(\varepsilon(\sigma) = \varepsilon(\rho)\), and a sum over \(\sigma\) is equivalent to a sum over \(\rho\).
\end{proof}

\subsection{Expanding by rows or columns}
For an \(n \times n\) matrix \(M\) with entries \(M_{ia}\), we define the minor \(M^{ia}\) to be the \((n-1)\times(n-1)\) determinant of the matrix obtained by deleting row \(i\) and column \(a\) from \(M\).
\begin{proposition}
	The determinant of a generic \(n \times n\) matrix \(M\) is given by
	\begin{align*}
		\det M
		 & = \sum_i (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed \(a\)} \\
		 & = \sum_a (-1)^{i+a} M_{ia} M^{ia} \text{ for a fixed \(i\)}
	\end{align*}
\end{proposition}
This process is known as expanding by row \(i\) or by column \(a\).
As an example, let us take the following \(4 \times 4\) complex matrix
\[
	M = \begin{pmatrix}
		i & 0  & 3  & 0  \\
		0 & 0  & 2i & 0  \\
		0 & 5i & 0  & -i \\
		2 & 0  & 0  & 1
	\end{pmatrix}
\]
Then, the determinant is given by (expanding by row 3)
\begin{align*}
	\det M
	 & = -5i\begin{vmatrix}
		i & 3  & 0 \\
		0 & 2i & 0 \\
		2 & 0  & 1
	\end{vmatrix} + i\begin{vmatrix}
		i & 0 & 3  \\
		0 & 0 & 2i \\
		2 & 0 & 0
	\end{vmatrix}                                                               \\
	 & = -5i\left[i\begin{vmatrix}
			2i & 0 \\
			0  & 1
		\end{vmatrix} - 3 \begin{vmatrix}
			0 & 0 \\
			2 & 1
		\end{vmatrix}\right] + i\left[-2i\begin{vmatrix}
			i & 0 \\
			2 & 0
		\end{vmatrix}\right] \\
	 & = -5i[i \cdot 2i - 3 \cdot 0] + i[-2i \cdot 0]                                                                             \\
	 & = -5i[-2] + i[0]                                                                                                           \\
	 & = 10i
\end{align*}

\subsection{Row and column operations}
Consider the following consequences of the properties of the determinant:
\begin{itemize}
	\item (row and column scaling) If \(\vb R_i \mapsto \lambda \vb R_i\) for a fixed \(i\), or \(\vb C_a \mapsto \lambda \vb C_a\), then \(\det M \mapsto \lambda \det M\) by multilinearity.
	      If we scale all rows or columns, then \(M \mapsto \lambda M\), so \(\det M \mapsto \lambda^n \det M\) where \(M\) is an \(n \times n\) matrix.
	\item (row and column operations) If \(\vb R_i \mapsto \vb R_i + \lambda \vb R_j\) where \(i \neq j\) (or the corresponding conversion with columns), then \(\det M \mapsto \det M\).
	\item (row and column exchanges) If we swap \(\vb R_i\) and \(\vb R_j\) (or two columns), then \(\det M \mapsto -\det M\).
\end{itemize}
For example, let us find the determinant of matrix \(A\), where
\[
	A = \begin{pmatrix}
		1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
	\end{pmatrix};\quad a \in \mathbb C
\]
Then:
\begin{align*}
	\det A                                                    & = \begin{vmatrix} 1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1 \end{vmatrix}                            \\
	\vb C_1 \mapsto \vb C_1 - \vb C_3:\quad \det A            & = \begin{vmatrix} 1-a & 1 & a \\ a-1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                            \\
	\det A                                                    & = (1-a)\begin{vmatrix} 1 & 1 & a \\ -1 & 1 & 1 \\ 0 & a & 1 \end{vmatrix}                       \\
	\vb C_2 \mapsto \vb C_2 - \vb C_3:\quad \det A            & = (1-a)\begin{vmatrix} 1 & 1-a & a \\ -1 & 0 & 1 \\ 0 & a-1 & 1 \end{vmatrix}                       \\
	\det A                                                    & = (1-a)^2\begin{vmatrix} 1 & 1 & a \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\vb R_1 \mapsto \vb R_1 + \vb R_2 + \vb R_3 :\quad \det A & = (1-a)^2\begin{vmatrix} 0 & 0 & a+2 \\ -1 & 0 & 1 \\ 0 & -1 & 1 \end{vmatrix}                     \\
	\det A                                                    & = (1-a)^2(a+2)\begin{vmatrix}-1&0\\0&-1\end{vmatrix} = (1-a)^2(a+2)
\end{align*}

\subsection{Multiplicative property of determinants}
\begin{theorem}
	For \(n\times n\) matrices \(M, N\), \(\det (MN) = \det M \cdot \det N\).
\end{theorem}
We can prove this using the following elaboration on the definition of the determinant:
\begin{lemma}
	\[
		\varepsilon_{i_1 i_2 \cdots i_n} M_{i_1 a_1} M_{i_2 a_2} \cdots M_{i_n a_n} = (\det M) \varepsilon_{a_1 a_2 \cdots a_n}
	\]
\end{lemma}
\begin{proof}
	The left hand side and right hand side are each totally antisymmetric (alternating) in \(a_1, a_2, \cdots, a_n\), so they must be related by a constant of proportionality.
	To fix the constant, we can simply consider taking \(a_i = i\) and the result follows.
\end{proof}
Now, we prove the above theorem.
\begin{proof}
	Using the lemma above:
	\begin{align*}
		\det MN & = \varepsilon_{i_1 i_2 \cdots i_n} (MN)_{i_1 1} (MN)_{i_2 2} \cdots (MN)_{i_n n}                                                    \\
		        & = \varepsilon_{i_1 i_2 \cdots i_n} {M_{i_1 k_1} \atop N_{k_1 1}} {M_{i_2 k_2} \atop N_{k_2 2}} \cdots {M_{i_n k_n} \atop N_{k_n n}} \\
		        & = (\det M) \varepsilon_{a_1 a_2 \cdots a_n} N_{k_1 1} N_{k_2 2} \cdots N_{k_n n}                                                    \\
		        & = (\det M)(\det N)
	\end{align*}
	as required.
\end{proof}

Note the following consequences.
\begin{enumerate}
	\item \(M^{-1}M = I \implies \det(M^{-1}) \det(M) = \det I = 1\).
	      Therefore, \(\det (M^{-1}) = (\det M)^{-1}\), so \(\det M\) must be nonzero for \(M\) to be invertible.
	\item For \(R\) real and orthogonal, \(R^\transpose R = I \implies \det(R^\transpose) \det(R) = 1\).
	      But \(\det (R^\transpose) = \det R\), so \((\det R)^2 = 1\), so \(\det R = \pm 1\).
	\item For \(U\) complex and unitary, \(U^\dagger U = I \implies \det(U^\dagger) \det(U) = 1\).
	      But since \(U^\dagger = \overline{U^\transpose}\), we have \(\overline{\det U} \det U = 1\), so \(\abs{(\det U)^2} = 1\), so \(\abs{\det U} = 1\).
\end{enumerate}

\subsection{Cofactors and determinants}
Consider a column of some \(n \times n\) matrix \(M\), written in the form
\[
	\vb C_a = \sum_i M_{ia} \vb e_i
\]
\begin{align*}
	\implies \det M & = [ \vb C_1, \cdots, \vb C_a, \cdots, \vb C_n ]                                         \\
	                & = [ \vb C_1, \cdots, \vb C_{a-1}, \sum_i M_{ia} \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ] \\
	                & = \sum_i M_{ia} \Delta_{ia}
\end{align*}
where
\begin{align*}
	\Delta_{ia} & = [ \vb C_1, \cdots, \vb C_{a-1}, \vb e_i, \vb C_{a+1}, \cdots, \vb C_n ]                                                                                         \\
	            & = \begin{vmatrix}
		\mathhuge A                & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge B                \\
		\begin{matrix}
			0 & \cdots & 0
		\end{matrix} & 1                          & \begin{matrix}
			0 & \cdots & 0
		\end{matrix} \\
		\mathhuge C                & \begin{matrix}
			0 \\ \vdots \\ 0
		\end{matrix} & \mathhuge D
	\end{vmatrix}
	\intertext{where the zero entries in the rows arise from antisymmetry, giving}
	            & = \underbrace{(-1)^{n-a}}_{\text{amount of column transpositions}} \cdot \underbrace{(-1)^{n-i}}_{\text{amount of row transpositions}} \begin{vmatrix}
		\mathhuge A & \mathhuge B \\
		\mathhuge C & \mathhuge D
	\end{vmatrix} \\
	            & = (-1)^{i+a}M^{ia}
\end{align*}
where \(M^{ia}\) is the minor in this position; the determinant of the matrix with this particular row and column removed.
We call \(\Delta_{ia}\) the cofactor.
\[
	\det M = \sum_i M_{ia} \Delta_{ia} = \sum_i(-1)^{i+a}M_{ia}M^{ia}
\]
Similarly, by considering rows,
\[
	\det M = \sum_a M_{ia} \Delta_{ia} = \sum_a(-1)^{i+a}M_{ia}M^{ia}
\]

\subsection{Adjugates and inverses}
Reasoning as above, consider \(\vb C_b = \sum_i M_{ib} \vb e_i\).
Then,
\[
	[\vb C_1, \cdots, \vb C_{a-1}, \vb C_b, \vb C_{a+1}, \cdots, \vb C_n ] = \sum_i M_{ib} \Delta_{ia}
\]
If \(a=b\) then clearly this is \(\det M\).
Otherwise, \(\vb C_b\) is equal to one of the other columns, so \(\sum_i M_{ib} \Delta_{ia} = 0\).
\[
	\sum_i M_{ib} \Delta_{ia} = (\det M)\delta_{ab}
\]
Similarly,
\[
	\sum_a M_{ja} \Delta_{ia} = (\det M)\delta_{ij}
\]
Now, let \(\Delta\) be the matrix of cofactors (i.e.\ entries \(\Delta_{ia}\)), and we define the adjugate \(\adjugate M = \Delta^\transpose\).
Then
\[
	\Delta_{ia}M_{ib} = \adjugate M_{ai}M_{ib} = (\adjugate M M)_{ab} = (\det M)\delta_{ab}
\]
Therefore,
\[
	\adjugate M M = (\det M) I
\]
We can reach this result similarly considering the other index.
Hence, if \(\det M \neq 0\) then \(M^{-1} = \frac{1}{\det M}\adjugate M\).

\subsection{Systems of linear equations}
Consider a system of \(n\) linear equations in \(n\) unknowns \(x_i\) written in matrix-vector form:
\[
	A\vb x = \vb b,\quad \vb x, \vb b \in \mathbb R^n,
\]
where \(A\) is an \(n \times n\) matrix.
There are three possibilities:
\begin{enumerate}
	\item \(\det A \neq 0 \implies A^{-1}\) exists so there is a unique solution \(\vb x = A^{-1} \vb b\)
	\item \(\det A = 0\) and \(b \notin \Im A\) means that there is no solution
	\item \(\det A = 0\) and \(b \in \Im A\) means that there are infinitely many solutions of the form
	      \[
		      \vb x = \vb x_0 + \vb u
	      \]
	      where \(\vb u \in \ker A\) and \(\vb x_0\) is a particular solution
\end{enumerate}
A solution therefore exists if and only if \(A\vb x_0 = \vb b\) for some \(\vb x_0\), which is true if and only if \(\vb b \in \Im A\).
Then \(\vb x\) is also a solution if and only if \(\vb u = \vb x - \vb x_0\) satisfies
\[
	A\vb u = \vb 0
\]
This equation is known as the equivalent homogeneous problem.
Now, \(\det A \neq 0 \iff \Im A = \mathbb R^n \iff \ker A = \{ \vb 0 \}\).
So in case (i), there is always a unique solution for any \(\vb b\).
But \(\det A = 0 \iff \rank(A) < n \iff \nullity A > 0\).
Then either \(b \notin \Im A\) as in case (ii), or \(b \in \Im A\) as in case (iii).

If \(\vb u_1, \dots, \vb u_k\) is a basis for \(\ker A\), then the general solution to the homogeneous problem is some linear combination of these basis vectors, i.e.
\[
	\vb u = \sum_{i=1}^k \lambda_i \vb u_i,\quad k = \nullity A
\]
This is similar to the complementary function and particular integral technique used to solve linear differential equations.

For example, in \(A\vb x = \vb b\), let
\[
	A = \begin{pmatrix}
		1 & 1 & a \\ a & 1 & 1 \\ 1 & a & 1
	\end{pmatrix};\quad \vb b = \begin{pmatrix}
		1 \\ c \\ 1
	\end{pmatrix};\quad a, c \in \mathbb R
\]
We have previously found that \(\det A = (a-1)^2(a+2)\).
So the cases are:
\begin{itemize}
	\item (\(a \neq 1, a \neq -2\)) \(\det A \neq 0\) and \(A^{-1}\) exists; we previously found this to be
	      \[
		      A^{-1} = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
			      1 & 1+a & 1 \\ 1 & 1 & -1-a \\ -1-a & 1 & 1
		      \end{pmatrix}
	      \]
	      For these values of \(a\), there is a unique solution for any \(c\), demonstrating case (i) above:
	      \[
		      \vb x = A^{-1} \vb b = \frac{1}{(1-a)(2+a)}\begin{pmatrix}
			      2-c-ca \\ c-a \\ c-a
		      \end{pmatrix}
	      \]
	      Geometrically, this solution is simply a point.
	\item (\(a = 1\)) In this case, the matrix is simply
	      \[
		      A = \begin{pmatrix}
			      1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1
		      \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\} = \left\{ \lambda\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\};\quad \ker A = \vecspan\left\{ \begin{pmatrix}
			      -1 \\ 1 \\ 0
		      \end{pmatrix}, \begin{pmatrix}
			      -1 \\ 0 \\ 1
		      \end{pmatrix} \right\}
	      \]
	      Note that \(\vb b \in \Im A\) if and only if \(c=1\), where a particular solution is
	      \[
		      \vb x_0 = \begin{pmatrix}
			      1 \\ 0 \\ 0
		      \end{pmatrix}
	      \]
	      So the general solution is given by
	      \[
		      \vb x = \vb x_0 + \vb u = \begin{pmatrix}
			      1 - \lambda - \mu \\ \lambda \\ \mu
		      \end{pmatrix}
	      \]
	      In summary, for \(a=1\), \(c=1\) we have case (iii).
	      Geometrically this is a plane.
	      For \(a=1\), \(c \neq 1\), we have case (ii) where there are no solutions.

	\item (\(a=-2\)) The matrix becomes
	      \[
		      A = \begin{pmatrix}
			      1 & 1 & -2 \\ -2 & 1 & 1 \\ 1 & -2 & 1
		      \end{pmatrix} \implies \Im A = \vecspan \left\{ \begin{pmatrix}
			      1 \\ -2 \\ 1
		      \end{pmatrix}, \begin{pmatrix}
			      1 \\ 1 \\ -2
		      \end{pmatrix} \right\};\quad \ker A = \left\{ \lambda\begin{pmatrix}
			      1 \\ 1 \\ 1
		      \end{pmatrix} \right\}
	      \]
	      Now, \(\vb b \in \Im A\) if and only if \(c = -2\), the particular solution is
	      \[
		      \vb x_0 = \begin{pmatrix}
			      1 \\ 0 \\ 0
		      \end{pmatrix}
	      \]
	      The general solution is therefore
	      \[
		      \vb x = \vb x_0 + \vb u = \begin{pmatrix}
			      1 + \lambda \\ \lambda \\ \lambda
		      \end{pmatrix}
	      \]
	      In summary, for \(a=-2\) and \(c=-2\) we have case (iii).
	      Geometrically this is a line.
	      For \(a=-2\), \(c \neq -2\), we have case (ii) where there are no solutions.
\end{itemize}

\subsection{Geometrical interpretation of solutions of linear equations}
Let \(\vb R_1, \vb R_2, \vb R_3\) be the rows of the \(3 \times 3\) matrix \(A\).
Then the rows represent the normals of planes.
This is clear by expanding the matrix multiplication of the homogeneous form:
\begin{align*}
	A\vb u = \vb 0 \iff & \vb R_1 \cdot \vb u = 0 \\
	                    & \vb R_2 \cdot \vb u = 0 \\
	                    & \vb R_3 \cdot \vb u = 0
\end{align*}
So the solution of the homogeneous problem (i.e.\ finding the general solution) amounts to determining where the planes intersect.
\begin{itemize}
	\item (\(\rank A = 3\)) The rows are linearly independent, so the three planes' normals are linearly independent and the planes intersect at \(\vb 0\) only.
	\item (\(\rank A = 2\)) The normals span a plane, so the planes intersect in a line.
	\item (\(\rank A = 1\)) The normals are parallel and therefore the planes coincide.
	\item (\(\rank A = 0\)) The normals are all zero, so any vector in \(\mathbb R^3\) solves the equation.
\end{itemize}
Now, let us consider instead the original problem \(A \vb x = \vb b\):
\begin{align*}
	A\vb b = \vb 0 \iff & \vb R_1 \cdot \vb u = b_1 \\
	                    & \vb R_2 \cdot \vb u = b_2 \\
	                    & \vb R_3 \cdot \vb u = b_3
\end{align*}
The planes still have normals \(\vb R_i\) as before, but they do not necessarily pass through the origin.
\begin{itemize}
	\item (\(\rank A = 3\)) The planes' normals are linearly independent and the planes intersect at a point; this is the unique solution.
	\item (\(\rank A < 3\)) The existence of a solution depends on the value of \(\vb b\).
	      \begin{itemize}
		      \item (\(\rank A = 2\)) The planes may intersect in a line as before, but they may instead form a sheaf (the planes pairwise intersect in lines but they do not as a triple), or two planes could be parallel and not intersect each other at all.
		      \item (\(\rank A = 1\)) The normals are parallel, so the planes may coincide or they might be parallel.
		            There is no solution unless all three planes coincide.
	      \end{itemize}
\end{itemize}
